{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d91d56f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4acc6c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import holoviews as hv\n",
        "from holoviews import opts\n",
        "\n",
        "hv.extension(\"matplotlib\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69595772",
      "metadata": {},
      "source": [
        "### Do Backprop with only Single Neuron Layers with Depth of 2 ( Stacked Linear Regression)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cbbbf44",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate data\n",
        "np.random.seed(0)\n",
        "n = 200\n",
        "\n",
        "x = np.linspace(-10, 10, 1000)\n",
        "noise = np.random.normal(size=len(x))\n",
        "\n",
        "y = 3 * x + 4 + noise\n",
        "\n",
        "actuals = hv.Scatter(zip(x, y))\n",
        "actuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d64fb8b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split into train and test sets.\n",
        "np.random.seed(0)\n",
        "\n",
        "# Create index for train and test that is randomly shuffled.\n",
        "idx = np.arange(len(x))\n",
        "np.random.shuffle(idx)\n",
        "split_pct = 0.80\n",
        "split_at = int(len(x) * split_pct)\n",
        "idx_train, idx_test = idx[slice(split_at)], idx[slice(-split_at)]\n",
        "\n",
        "# Split into train and test sets.\n",
        "x_train, y_train = x[idx_train].copy(), y[idx_train].copy()\n",
        "x_test, y_test = x[idx_test].copy(), y[idx_test].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53e96cf2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create standard scalers for train x and y.\n",
        "def standard_scaler(arr):\n",
        "    mean = arr.mean()\n",
        "    std = arr.std()\n",
        "\n",
        "    def ss(vv):\n",
        "        return (vv - mean) / std\n",
        "\n",
        "    def ss_inv(vv):\n",
        "        return (vv * std) + mean\n",
        "\n",
        "    return ss, ss_inv\n",
        "\n",
        "\n",
        "ss_x, ss_x_inv = standard_scaler(x_train)\n",
        "ss_y, ss_y_inv = standard_scaler(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f340d6fb",
      "metadata": {},
      "source": [
        "Given the cost function:\n",
        "$J(\\hat{y}) = \\frac{1}{n}\\sum_{i=0}^{n} (y_i - \\hat{y}_i)^2$  \n",
        "Where $\\hat{y} = (W \\cdot z + \\beta)$  \n",
        "Where $z$ is the previous activation in a layer and for $W_0 = X$\n",
        "\n",
        "The gradient of the cost function $J(\\hat{y})$ w.r.t. the Weights $W$ and Biases $\\beta$ gives us\n",
        "$$ \\frac{dJ(\\hat{y})}{dW} = \\frac{-2}{n}\\sum_{i=0}^{n} (y_i - \\hat{y_i})z_i$$  \n",
        "$$ \\frac{dJ(\\hat{y})}{d\\beta} = \\frac{-2}{n}\\sum_{i=0}^{n} (y_i - \\hat{y_i}) $$  \n",
        "\n",
        "It's important to note that the loss w.r.t the weights of layer j $W_j$ are are not influences by layer j.  Rather they are influenced by the activiation directly prior to layer $W_j$  \n",
        "\n",
        "In The case that there is only one layer of weights then the previous activation is just the input data $X$.\n",
        "In other words such a scenario is just a linear regression.\n",
        "$$ \\frac{dJ(\\hat{y})}{dW} = \\frac{-2}{n}\\sum_{i=0}^{n} (y_i - \\hat{y_i})X_i$$  \n",
        "$$ \\frac{dJ(\\hat{y})}{d\\beta} = \\frac{-2}{n}\\sum_{i=0}^{n} (y_i - \\hat{y_i}) $$  "
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40f462ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "class StackedLinReg2Deep:\n",
        "    def __init__(self, learning_rate=0.001):\n",
        "        # Intialiaze learning rate.\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Intialize weights and biases.\n",
        "        self.w0 = np.random.random(1)[0]\n",
        "        self.b0 = np.random.random(1)[0]\n",
        "        self.w1 = np.random.random(1)[0]\n",
        "        self.b1 = np.random.random(1)[0]\n",
        "\n",
        "        # Initialize loss.\n",
        "        self.j = np.inf\n",
        "\n",
        "        # Initialize gradients.\n",
        "        self.dj_wrt_dw0 = 0\n",
        "        self.dj_wrt_db0 = 0\n",
        "        self.dj_wrt_dw1 = 0\n",
        "        self.dj_wrt_db1 = 0\n",
        "\n",
        "    # fwd computes each foward step through network.\n",
        "    def fwd(self, x):\n",
        "        z0 = x * self.w0 + self.b0\n",
        "        z1 = z0 * self.w1 + self.b1\n",
        "        return np.array([x, z0, z1])\n",
        "\n",
        "    # bwd computes the gradient of the cost function given z (forward propogation\n",
        "    # vector) and y.\n",
        "    def bwd(self, z, y):\n",
        "        # Split up z into each forward pass result.\n",
        "        x, z0, z1 = z\n",
        "\n",
        "        # Compute loss.\n",
        "        # self.j = np.power(y - (self.w1 * (x * self.w0 + b0) + b1), 2).sum()\n",
        "        self.j = np.power(y - z1, 2).mean()\n",
        "\n",
        "        # Compute derivative of last layer.\n",
        "        self.dj_wrt_dw1 = 2 * (z0 * (y - z1)).mean()\n",
        "        self.dj_wrt_db1 = 2 * (y - z1).mean()\n",
        "\n",
        "        # Calculate what the activation should have been for the last layer.\n",
        "        z_hat = (y - self.b1) / self.w1\n",
        "\n",
        "        self.dj_wrt_dw0 = 2 * (x * (z_hat - z0)).mean()\n",
        "        # Compute derivative of first layer.\n",
        "        self.dj_wrt_db0 = 2 * (z_hat - z0).mean()\n",
        "\n",
        "    # step updates the weights and biases based from the gradient.\n",
        "    def step(self):\n",
        "        self.w0 += self.learning_rate * self.dj_wrt_dw0\n",
        "        self.b0 += self.learning_rate * self.dj_wrt_db0\n",
        "        self.w1 += self.learning_rate * self.dj_wrt_dw1\n",
        "        self.b1 += self.learning_rate * self.dj_wrt_db1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89710e77",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, x, y, epochs, early_stopping=False, epsilon=0.000001):\n",
        "    # Initialize the previous loss for early stopping.\n",
        "    loss_prev = model.j\n",
        "    # fwd, bwd, and step for each epoch.\n",
        "    for i in range(epochs):\n",
        "        # Foward pass.\n",
        "        z = model.fwd(x)\n",
        "\n",
        "        # Backward pass.\n",
        "        model.bwd(z, y)\n",
        "        print(f\"{i:08}> loss (j): {model.j:>7f}\")\n",
        "\n",
        "        # Setup early stopping.\n",
        "        if (loss_prev < model.j or model.j < epsilon) and early_stopping:\n",
        "            break\n",
        "        loss_prev = model.j\n",
        "\n",
        "        # Update weights and biases.\n",
        "        model.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0726dc79",
      "metadata": {},
      "outputs": [],
      "source": [
        "def pred_plot(x, y, scale_x, scale_inv_pred, title, model):\n",
        "    preds = scale_inv_pred(model.fwd(scale_x(x))[-1])\n",
        "    return hv.Scatter(zip(x, y)) * hv.Scatter(zip(x, preds)).opts(title=title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a0606d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "stacked_lin_reg = StackedLinReg2Deep(learning_rate=0.2)\n",
        "train(stacked_lin_reg, ss_x(x_train), ss_y(y_train), 20000, early_stopping=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38b1dc95",
      "metadata": {},
      "outputs": [],
      "source": [
        "hv.Layout(\n",
        "    [\n",
        "        pred_plot(\n",
        "            x_train, y_train, ss_x, ss_y_inv, \"Train Predictions\", stacked_lin_reg\n",
        "        ),\n",
        "        pred_plot(x_test, y_test, ss_x, ss_y_inv, \"Test Predictions\", stacked_lin_reg),\n",
        "        pred_plot(x, y, ss_x, ss_y_inv, \"All Predictions\", stacked_lin_reg),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4eb6bc73",
      "metadata": {},
      "source": [
        "## Now to Generalize the Number of Layers"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06d25dbd",
      "metadata": {},
      "outputs": [],
      "source": [
        "class StackedLinReg:\n",
        "    def __init__(self, n_layers, learning_rate=0.001):\n",
        "        # Intialiaze learning rate.\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Intialize weights and biases.\n",
        "        self.W = np.random.random(n_layers)\n",
        "        self.B = np.random.random(n_layers)\n",
        "\n",
        "        # Initialize loss.\n",
        "        self.j = np.inf\n",
        "\n",
        "        # Initialize gradients.\n",
        "        self.dj_wrt_dW = np.zeros(n_layers)\n",
        "        self.dj_wrt_db = np.zeros(n_layers)\n",
        "\n",
        "    # fwd computes each foward step through network.\n",
        "    def fwd(self, x):\n",
        "        z = np.zeros((self.n_layers + 1, len(x)))\n",
        "        z[0] = x\n",
        "        for i in range(self.n_layers):\n",
        "            z[i + 1] = z[i] * self.W[i] + self.B[i]\n",
        "        return z\n",
        "\n",
        "    # bwd computes the gradient of the cost function given z (forward propogation\n",
        "    # vector) and y.\n",
        "    def bwd(self, z, y):\n",
        "        # Compute loss.\n",
        "        self.j = np.power(y - z[-1], 2).mean()\n",
        "\n",
        "        # z_hat is what the activation should have been.\n",
        "        z_hat = y\n",
        "        # Back propogate the activations through each layer.\n",
        "        for i in range(self.n_layers):\n",
        "            # Take the gradient with respect to the W and b\n",
        "            self.dj_wrt_dW[i] = 2 * (z[i] * (z_hat - z[i + 1])).mean()\n",
        "            self.dj_wrt_db[i] = 2 * (z_hat - z[i + 1]).mean()\n",
        "            # Update correct activation given current layers' W and b.\n",
        "            z_hat = (z_hat - self.B[i]) / self.W[i]\n",
        "\n",
        "    # step updates the weights and biases based from the gradient.\n",
        "    def step(self):\n",
        "        self.W += self.learning_rate * self.dj_wrt_dW\n",
        "        self.B += self.learning_rate * self.dj_wrt_db\n",
        "\n",
        "    @property\n",
        "    def n_layers(self):\n",
        "        return len(self.W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1a3ee75",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "stacked_lin_reg2 = StackedLinReg(3, learning_rate=0.05)\n",
        "train(stacked_lin_reg2, ss_x(x_train), ss_y(y_train), 20000, early_stopping=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e41732f",
      "metadata": {},
      "outputs": [],
      "source": [
        "hv.Layout(\n",
        "    [\n",
        "        pred_plot(\n",
        "            x_train, y_train, ss_x, ss_y_inv, \"Train Predictions\", stacked_lin_reg2\n",
        "        ),\n",
        "        pred_plot(x_test, y_test, ss_x, ss_y_inv, \"Test Predictions\", stacked_lin_reg2),\n",
        "        pred_plot(x, y, ss_x, ss_y_inv, \"All Predictions\", stacked_lin_reg2),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ca9a71b",
      "metadata": {},
      "source": [
        "Given the cost function:  \n",
        "\n",
        "$J(\\hat{y}) = \\frac{1}{n}\\sum_{i=0}^{n} (y_i - \\hat{y}_i)^2$  \n",
        "\n",
        "Where $f$ is the activation function.  \n",
        "Where $\\hat{y} = f(W \\cdot z + \\beta)$  \n",
        "Where $z$ is the previous activation in a layer and for $W_0 = X$\n",
        "\n",
        "The gradient of the cost function $J(\\hat{y})$ w.r.t. the Weights $W$ and Biases $\\beta$ gives us\n",
        "$$ \\frac{dJ(\\hat{y})}{dW} = \\frac{-2}{n}\\sum_{i=0}^{n} (y_i - \\hat{y_i})f^{'}(z_i)z_i$$  \n",
        "$$ \\frac{dJ(\\hat{y})}{d\\beta} = \\frac{-2}{n}\\sum_{i=0}^{n} (y_i - \\hat{y_i})f^{'}(z_i)$$  "
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "01213350",
      "metadata": {},
      "source": [
        "Take the example of taking the gradient of the loss w.r.t. each weight $W$ in each layer for a NN with 3 layers 0, 1, and 2.\n",
        "\n",
        "$$\\hat{y} = z_2$$\n",
        "$$z_2 = f( W_2 z_1 + \\beta_2 )$$\n",
        "$$z_1 = f( W_1 z_0 + \\beta_1 )$$\n",
        "$$z_0 = f( W_0 x + \\beta_0 )$$\n",
        "\n",
        "Layer2:  \n",
        "$$ \\frac{dJ(\\hat{y})}{dW_2} = (y_i - \\hat{y_i}) f'( W_2 z_1 + \\beta_2 ) z_1 $$\n",
        "\n",
        "Layer1:  \n",
        "$$ \\frac{dJ(\\hat{y})}{dW_1} = (y_i - \\hat{y_i}) f'( W_2 z_1 + \\beta_2 ) ( W_2 ( f'( W_1 z_0 + \\beta_1 ) z_0 ) ) $$\n",
        "\n",
        "Layer0:  \n",
        "$$ \\frac{dJ(\\hat{y})}{dW_1} = (y_i - \\hat{y_i}) f'( W_2 z_1 + \\beta_2 ) ( W_2 ( f'( W_1 z_0 + \\beta_1 ) ( w_1 f'( w_0 x_0 + b_0 ) x_0 ) ) ) $$\n",
        "\n",
        "\n",
        "Organizing the derivaties differently we get:  \n",
        "$ \\frac{dJ(\\hat{y})}{dW_2} = (y_i - \\hat{y_i}) f'( W_2 z_1 + \\beta_2 ) z_1 $  \n",
        "$ \\frac{dJ(\\hat{y})}{dW_1} = (y_i - \\hat{y_i}) f'( W_2 z_1 + \\beta_2 ) f'( W_1 z_0 + \\beta_1 ) W_2 z_0 $  \n",
        "$ \\frac{dJ(\\hat{y})}{dW_0} = (y_i - \\hat{y_i}) f'( W_2 z_1 + \\beta_2 ) f'( W_1 z_0 + \\beta_1 ) f'( w_0 x_0 + b_0 ) W_2 w_1 x_0 $  "
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "e915e411",
      "metadata": {},
      "source": [
        "The gradient of the loss w.r.t the biases $\\beta$ is the same just without the previous activation $z_{i-1}$ included.  \n",
        "$ \\frac{dJ(\\hat{y})}{d\\beta_2} = (y_i - \\hat{y_i}) f'( W_2 z_1 + \\beta_2 )$  \n",
        "$ \\frac{dJ(\\hat{y})}{d\\beta_1} = (y_i - \\hat{y_i}) f'( W_2 z_1 + \\beta_2 ) f'( W_1 z_0 + \\beta_1 ) W_2$  \n",
        "$ \\frac{dJ(\\hat{y})}{d\\beta_0} = (y_i - \\hat{y_i}) f'( W_2 z_1 + \\beta_2 ) f'( W_1 z_0 + \\beta_1 ) f'( w_0 x_0 + b_0 ) W_2 w_1$  "
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "0872f9ea",
      "metadata": {},
      "source": [
        "This means that we need to store both the imediate activation $Z$ e.g. $f(W_i z_{i-1} + \\beta_i)$ and all the partials that look like $f'(W_i z_{i-1} + \\beta_i)$ for each layer after layer $i$.  \n",
        "Symbolically this means:  \n",
        "<!-- let $i$ be the current layer   -->\n",
        "<!-- let $F'_i $ be the set of partials needed to be stored for layer $i$   -->\n",
        "<!-- $ \\{ F'_i | i \\epsilon W, f(W_i z_{i-1} + \\beta_i) \\} $  -->\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0404bf2",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Activation:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def act(self, arr):\n",
        "        return arr\n",
        "\n",
        "    def grd(self, arr):\n",
        "        return np.ones(arr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "579d60b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Sigmoid(Activation):\n",
        "    def __init__(self, clip=10):\n",
        "        self.clip = clip\n",
        "\n",
        "    def act(self, arr):\n",
        "        arr_ = np.clip(arr, -self.clip, self.clip)\n",
        "        # return np.power(1 + np.exp(-arr), -1)\n",
        "        return np.power(1 + np.exp(-arr_), -1)\n",
        "\n",
        "    def grd(self, arr):\n",
        "        arr_ = np.clip(arr, -self.clip, self.clip)\n",
        "        # return np.exp(-arr) * np.power(1 + np.exp(-arr), -2)\n",
        "        e_neg_x = np.exp(-arr_)\n",
        "        return e_neg_x * np.power(1 + e_neg_x, -2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd18cbb1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "\n",
        "class StackedNonLinReg:\n",
        "    def __init__(self, n_layers, learning_rate=0.001):\n",
        "        # Intialiaze learning rate.\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Intialize weights and biases.\n",
        "        self.W = np.random.random(n_layers)\n",
        "        self.B = np.random.random(n_layers)\n",
        "\n",
        "        # Initialize activations\n",
        "        self.activations: List[Activation] = [\n",
        "            Activation() if (i != 1) else Sigmoid() for i in range(n_layers)\n",
        "        ]\n",
        "\n",
        "        # Initialize loss.\n",
        "        self.j = np.inf\n",
        "\n",
        "        # Initialize gradients.\n",
        "        self.dj_wrt_dW = np.zeros(n_layers)\n",
        "        self.dj_wrt_dB = np.zeros(n_layers)\n",
        "\n",
        "    # fwd computes each foward step through network.\n",
        "    def fwd(self, x):\n",
        "        # Set up the activations setting the 0th activation to be the input data.\n",
        "        z = [None for _ in range(self.n_layers + 1)]\n",
        "        z[0] = x\n",
        "        # Calculate each activation by doing activation of W * z + B.\n",
        "        for i in range(self.n_layers):\n",
        "            activation = self.activations[i]\n",
        "            z[i + 1] = activation.act(z[i] * self.W[i] + self.B[i])\n",
        "        return z\n",
        "\n",
        "    # bwd computes the gradient of the cost function given z (forward propogation\n",
        "    # vector) and y.\n",
        "    def bwd(self, z, y):\n",
        "        # Compute loss and partial loss.\n",
        "        self.j = np.power(y - z[-1], 2).mean()\n",
        "        loss_partial = y - z[-1]\n",
        "\n",
        "        # Calculate partial gradients: f'(W_i * z_(i-1)+b_i) for each layer.\n",
        "        part_grd = [None for _ in range(self.n_layers)]\n",
        "        for i in range(self.n_layers):\n",
        "            part_grd[i] = self.activations[i].grd(z[i])\n",
        "\n",
        "        # z_hat is what the activation should have been.\n",
        "        for i in range(self.n_layers):\n",
        "            # Take the prodect of partial gradients activations [0-i].\n",
        "            part_grd_prod = np.ones(part_grd[0].shape)\n",
        "            for j in range(i + 1):\n",
        "                part_grd_prod = part_grd_prod * part_grd[i]\n",
        "\n",
        "            # Mutliply weights [i+1-n_layers] together.\n",
        "            weight_prod = 1\n",
        "            for j in range(i + 1, self.n_layers):\n",
        "                weight_prod = weight_prod * self.W[i]\n",
        "\n",
        "            # Calculate the gradient w.r.t. wieghts and biases\n",
        "            dj_wrt_dB = part_grd_prod * weight_prod * loss_partial\n",
        "            self.dj_wrt_dB[i] = dj_wrt_dB.mean()\n",
        "            self.dj_wrt_dW[i] = (dj_wrt_dB * z[0]).mean()\n",
        "\n",
        "    # step updates the weights and biases based from the gradient.\n",
        "    def step(self):\n",
        "        self.W += self.learning_rate * self.dj_wrt_dW\n",
        "        self.B += self.learning_rate * self.dj_wrt_dB\n",
        "\n",
        "    @property\n",
        "    def n_layers(self):\n",
        "        return len(self.W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f78c6460",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "stacked_non_lin_reg = StackedNonLinReg(5, learning_rate=0.008)\n",
        "train(stacked_non_lin_reg, ss_x(x_train), ss_y(y_train), 400, early_stopping=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e3107f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "hv.Layout(\n",
        "    [\n",
        "        pred_plot(\n",
        "            x_train, y_train, ss_x, ss_y_inv, \"Train Predictions\", stacked_non_lin_reg\n",
        "        ),\n",
        "        pred_plot(\n",
        "            x_test, y_test, ss_x, ss_y_inv, \"Test Predictions\", stacked_non_lin_reg\n",
        "        ),\n",
        "        pred_plot(x, y, ss_x, ss_y_inv, \"All Predictions\", stacked_non_lin_reg),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d45cdd69",
      "metadata": {},
      "source": [
        "### Now to Stack Layers that have Multiple Dimensions to Them"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "988f29c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "class RELU(Activation):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def act(self, arr):\n",
        "        return np.maximum(0, arr)\n",
        "\n",
        "    def grd(self, arr):\n",
        "        return (arr != 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a30f98fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "class LeakyRELU(Activation):\n",
        "    def __init__(self, neg_slope=0.05):\n",
        "        self.neg_slope = neg_slope\n",
        "\n",
        "    def act(self, arr):\n",
        "        lt_z = (arr < 0)\n",
        "        return (lt_z * self.neg_slope + (~lt_z)) * arr\n",
        "        \n",
        "\n",
        "    def grd(self, arr):\n",
        "        lt_z = arr < 0\n",
        "        return lt_z * self.neg_slope +  (~lt_z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a95cb80",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "\n",
        "class StackedMultiNonLinReg:\n",
        "    def __init__(\n",
        "        self,\n",
        "        layers: List[int],\n",
        "        learning_rate=0.001,\n",
        "    ):\n",
        "        # Intialize learning rate.\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize weights and biases.\n",
        "        self.B = [np.random.random(layer) for layer in layers[1:]]\n",
        "        self.W = [\n",
        "            np.random.random((layers[i], layers[i + 1])) for i in range(len(layers) - 1)\n",
        "        ]\n",
        "\n",
        "        # Initialize activations\n",
        "        self.activations: List[Activation] = [\n",
        "            Activation() for i in range(self.n_layers)\n",
        "#             Activation() if (i != 2) else Sigmoid() for i in range(self.n_layers)\n",
        "        ]\n",
        "\n",
        "        # Initialize loss.\n",
        "        self.j = np.inf\n",
        "\n",
        "        # Initialize gradients.\n",
        "        self.dj_wrt_dW = [np.zeros(W.shape) for W in self.W]\n",
        "        self.dj_wrt_dB = [np.zeros(B.shape) for B in self.B]\n",
        "\n",
        "    # fwd computes each foward step through network.\n",
        "    def fwd(self, x):\n",
        "        # Set up the activations setting the 0th activation to be the input data.\n",
        "        z = [None for _ in range(self.n_layers + 1)]\n",
        "        z[0] = x\n",
        "        # Calculate each activation by doing activation of W * z + B.\n",
        "        for i in range(self.n_layers):\n",
        "            activation = self.activations[i]\n",
        "            z[i + 1] = activation.act(np.matmul(z[i], self.W[i]) + self.B[i])\n",
        "        return z\n",
        "\n",
        "    # bwd computes the gradient of the cost function given z (forward propogation\n",
        "    # vector) and y.\n",
        "    def bwd(self, z, y):\n",
        "        # Compute loss and partial loss.\n",
        "        self.j = np.power(y - z[-1], 2).mean()\n",
        "        loss_partial = y - z[-1]\n",
        "\n",
        "        # Calculate partial gradients: f'(W_i * z_(i-1)+b_i) for each layer.\n",
        "        part_grd = [None for _ in range(self.n_layers)]\n",
        "        for i in range(self.n_layers):\n",
        "            part_grd[i] = self.activations[i].grd(z[i])\n",
        "\n",
        "        # z_hat is what the activation should have been.\n",
        "        for i in range(self.n_layers):\n",
        "            # Take the prodect of partial gradients activations [0-i].\n",
        "            part_grd_prod = np.ones(part_grd[0].shape)\n",
        "            for j in range(i + 1):\n",
        "                part_grd_prod = part_grd_prod * part_grd[i]\n",
        "\n",
        "            # Mutliply weights [i+1-n_layers] together.\n",
        "            weight_prod = np.ones((self.W[i].shape[0], 1))\n",
        "            for j in range(i + 1, self.n_layers):\n",
        "                weight_prod = weight_prod * self.W[i]\n",
        "\n",
        "            # Calculate the gradient w.r.t. wieghts and biases\n",
        "            dj_wrt_dB = np.matmul(part_grd_prod, weight_prod) * loss_partial\n",
        "\n",
        "            self.dj_wrt_dB[i] = dj_wrt_dB.mean()\n",
        "            self.dj_wrt_dW[i] = (dj_wrt_dB * z[0]).mean()\n",
        "\n",
        "    # step updates the weights and biases based from the gradient.\n",
        "    def step(self):\n",
        "        for i in range(self.n_layers):\n",
        "            self.W[i] += self.learning_rate * self.dj_wrt_dW[i]\n",
        "            self.B[i] += self.learning_rate * self.dj_wrt_dB[i]\n",
        "\n",
        "    @property\n",
        "    def n_layers(self):\n",
        "        return len(self.W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "223d2d94",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "stacked_multi_non_lin_reg = StackedMultiNonLinReg([1, 2, 1], learning_rate=0.1)\n",
        "\n",
        "print(\"Weights Shapes:\", list(map(lambda x: x.shape, stacked_multi_non_lin_reg.W)))\n",
        "print(\"Activations Shapes:\", stacked_multi_non_lin_reg.activations)\n",
        "\n",
        "train(\n",
        "    stacked_multi_non_lin_reg,\n",
        "    ss_x(x_train)[..., np.newaxis],\n",
        "    ss_y(y_train)[..., np.newaxis],\n",
        "    25,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f10b65d",
      "metadata": {},
      "outputs": [],
      "source": [
        "hv.Layout(\n",
        "    [\n",
        "        pred_plot(\n",
        "            x_train[..., np.newaxis],\n",
        "            y_train[..., np.newaxis],\n",
        "            ss_x,\n",
        "            ss_y_inv,\n",
        "            \"Train Predictions\",\n",
        "            stacked_multi_non_lin_reg,\n",
        "        ),\n",
        "        pred_plot(\n",
        "            x_test[..., np.newaxis],\n",
        "            y_test[..., np.newaxis],\n",
        "            ss_x,\n",
        "            ss_y_inv,\n",
        "            \"Test Predictions\",\n",
        "            stacked_multi_non_lin_reg,\n",
        "        ),\n",
        "        pred_plot(\n",
        "            x[..., np.newaxis],\n",
        "            y[..., np.newaxis],\n",
        "            ss_x,\n",
        "            ss_y_inv,\n",
        "            \"All Predictions\",\n",
        "            stacked_multi_non_lin_reg,\n",
        "        ),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e2337f0",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
